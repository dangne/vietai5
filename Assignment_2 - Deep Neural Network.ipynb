{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"},"colab":{"name":"Assignment_2 - Deep Neural Network.ipynb","provenance":[],"collapsed_sections":[]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"jAY-HQLVVK05","colab_type":"text"},"source":["## Assignment 2: Deep Neural Network \n"]},{"cell_type":"markdown","metadata":{"id":"fciXmk4mVK06","colab_type":"text"},"source":["### Giới thiệu\n","\n","Để có thể hoàn tất bài tập này, các bạn cần nắm rõ những kiến thức sau:\n","\n","    - Neural Networks - Fully connected networks là gì, nguyên tắc hoạt động ra sao.\n","\t- Giải thuật Feedforward và BackPropagation trong bài toán NN.\n","\t- Giải thuật gradient descent - Batch and Mini-batch.\n","\t- Regularization để tránh overfitting trong NN.\n","\n","Các bạn có thể tham khảo lại bài giảng của lớp để nắm vững các nội dung này. Ngoài ra, các bạn có thể đặt câu hỏi cho đội ngũ giảng dạy nếu có thắc mắc. \n","\n","Trong bài tập này các bạn sẽ sử dụng Neural Networks để giải quyết 2 bài toán:\n","\n","\t- Bài 1: phân loại dữ liệu BAT, gồm 3 lớp.\n","![Dữ liệu 3 class BAT](https://i.imgur.com/d1Pd1XT.png)\n","\t- Bài 2: phân loại tập fashion MNIST, gồm 10 lớp.\n","![Dữ liệu Fashion MNIST](https://i.imgur.com/O9dqdId.png)\n","Yêu cầu dành cho các bạn trong là giải quyết hai bài trên bằng Numpy và TensorFlow.\n","\n","Mục tiêu của bài tập lần này là hiện thực Neural Networks mạng Fully Connected một cách cơ bản trên Numpy và Tensorflow. Một mạng cơ bản sẽ gồm nhiều hidden layers và một lớp softmax tại layer cuối cùng phù hợp cho việc phân loại dữ liệu. \n","\n","![Mạng neural network. Nguồn: graphicsminer.com/neuralnetwork](https://i.imgur.com/K3Yvt20.png)\n","\n","Khi thiết kế một mạng cơ bản thì người dùng có thể quyết định số input feature cho tầng input. Số output sẽ là số lớp mà người đó muốn phân loại. Ví dụ như bài toán fashion MNIST thì số feature đầu vào chính bằng số pixel của mỗi ảnh, số nút đầu ra sẽ bằng số lớp cần phân loại (10). Đối với số lượng hidden layer và số lượng nodes tương ứng, ta có thể tùy chọn.\n","\n","Một chú ý rất quan trọng là số nodes đầu ra của layer trước sẽ là số inputs đầu vào của layers sau đó.\n"]},{"cell_type":"markdown","metadata":{"id":"rPH8uEJCVK07","colab_type":"text"},"source":["## I. Thực hiện Deep Neural Network với Numpy\n","### Những công việc bạn phải thực hiện \n","\n","1. Các hàm activation `sigmoid`, `tanh`, `relu`, `softmax` và đạo hàm của nó `sigmoid_grad`, `tanh_grad`, `relu_grad`.\n","2. Hàm `forward` và `backward` ở class `HiddenLayer`\n","```python\n","    class HiddenLayer:\n","    \n","        def forward(self, X):\n","            ...\n","    \n","        def backward(self, X, delta_prev):\n","            ...\n","```\n","3. Hàm `forward`, `backward`, `compute_loss` ở class `NeuralNetwork`\n","```python\n","    class NeuralNetwork:\n","        \n","        def forward(self, X):\n","            ...\n","            \n","        def backward(self, X, Y, layers):\n","            ...\n","            \n","        def compute_loss(self, Y, Y_hat):\n","            ...\n","```"]},{"cell_type":"markdown","metadata":{"id":"TohoN6f1VK07","colab_type":"text"},"source":["### Ký hiệu\n","\n","- $L$: số layers trong mạng neural network. \n","- $l = 0,1,..,L$ với $0$ là layer input và $L$ layer output.\n","- $n^{[l]}$ là số neurons tại layer $l$\n","- $l-1$: layer trước theo chiều forward của $l$.\n","- $l+1$: layer trước theo chiều backward của $l$.\n","- $\\sigma'(x)$: đạo hàm hàm activation theo x (general case cho cả đạo hàm sigmoid, tanh, relu).\n","- $Z^{[l]}$: linear function values tại layer $l$.\n","- $A^{[l]}$: activation function values tại layer $l$."]},{"cell_type":"markdown","metadata":{"id":"dneL9wNuVK08","colab_type":"text"},"source":["### Import các thư viện cần thiết"]},{"cell_type":"markdown","metadata":{"id":"He9JhemNND3D","colab_type":"text"},"source":["**Chú ý:** Nếu bạn chạy trên Google Colab thì các thư viện này đã được tích hợp sẵn. Nếu bạn chạy trên máy cá nhân, bạn cần install các thư viện *numpy*, *matplotlib*, *googledrivedownloader*, *sklearn*."]},{"cell_type":"code","metadata":{"id":"z5pqVHsNVK08","colab_type":"code","colab":{}},"source":["%matplotlib inline\n","\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from IPython import display\n","from google_drive_downloader import GoogleDriveDownloader as gdd\n","import os\n","import sys\n","from sklearn.metrics import confusion_matrix"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qGdoufr7VK0-","colab_type":"text"},"source":["### Download dữ liệu và các utility functions"]},{"cell_type":"code","metadata":{"id":"2GG7Zj3aVK0_","colab_type":"code","outputId":"02ef76ee-e751-4014-cf6f-c4df25ef21b6","executionInfo":{"status":"ok","timestamp":1569296887053,"user_tz":-420,"elapsed":20899,"user":{"displayName":"Dang Nguyen Minh","photoUrl":"","userId":"06404449821748651554"}},"colab":{"base_uri":"https://localhost:8080/","height":119}},"source":["# DOWNLOAD DATA\n","gdd.download_file_from_google_drive(file_id='10W7c6WScnTeFbh2wx3kyVbzGg07B5QwE', \n","                                    dest_path=os.path.join(os.getcwd(), 'assignment2.zip'), unzip=True)\n","\n","if sys.platform.startswith(\"win\"):\n","    !move \"./assignment2/data\" \".\"\n","    !move \"./assignment2/test\" \".\"\n","    !move \"./assignment2/utils\" \".\"\n","    !del \"assignment2.zip\"\n","    !rd /s /q \"assignment2\" \"__MACOSX\"\n","    !dir\n","else:\n","    !mv assignment2/* .\n","    !rm assignment2.zip\n","    !rm -rf assignment2 __MACOSX\n","    # SHOW THE ITEMS OF CURRENT DIRRECTORY\n","    !ls"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Downloading 10W7c6WScnTeFbh2wx3kyVbzGg07B5QwE into /content/assignment2.zip... Done.\n","Unzipping...Done.\n","mv: cannot move 'assignment2/data' to './data': Directory not empty\n","mv: cannot move 'assignment2/test' to './test': Directory not empty\n","mv: cannot move 'assignment2/utils' to './utils': Directory not empty\n","data  sample_data  test  utils\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ctdckAhpYkqf","colab_type":"text"},"source":["### Import utility functions"]},{"cell_type":"code","metadata":{"id":"sJeV7fvvYtl1","colab_type":"code","colab":{}},"source":["from utils.util import *\n","from utils.gradient_check import *"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PVLd-jBpVK1B","colab_type":"text"},"source":["### Các hàm activation\n","\n","$$sigmoid(x) = \\frac{1}{1+e^{-x}}$$\n","\n","$$tanh(x) = \\frac{e^{x} - e^{-x}}{e^{x} + e^{-x}} $$\n","\n","$$relu(x) = \\begin{cases} x, & \\mbox{if } x > 0 \\\\ 0, & \\mbox{if } x <= 0 \\end{cases}$$\n","\n","$$sigmoid'(x) = sigmoid(x)(1 - sigmoid(x)) $$\n","\n","$$tanh'(x) = 1 - tanh^2(x)$$\n","\n","$$relu'(x) = \\begin{cases} 1, & \\mbox{if } x > 0 \\\\ 0, & \\mbox{if } x <= 0 \\end{cases}$$"]},{"cell_type":"markdown","metadata":{"id":"uQxzv26Lo1a_","colab_type":"text"},"source":["#### \\[TODO 1\\] Cài đặt các hàm activation\n","Định nghĩa các hàm activation ở cell bên dưới. (1đ)"]},{"cell_type":"code","metadata":{"id":"hI_pLSHVVK1C","colab_type":"code","colab":{}},"source":["def sigmoid(x):\n","    \"\"\"\n","    Sigmoid function.\n","    :param x: input\n","    \"\"\"\n","    #### [TODO 1] START CODE HERE #### \n","    x = 1 / (1 + np.exp(-x))\n","    #### END CODE HERE ####\n","    return x\n","\n","\n","def sigmoid_grad(x):\n","    \"\"\"\n","    Compute gradient of sigmoid.\n","    :param x: input\n","    \"\"\"\n","    \n","    #### [TODO 1] START CODE HERE #### \n","    da = sigmoid(x)*(1 - sigmoid(x))\n","    #### END CODE HERE ####\n","    return da\n","\n","\n","def relu(x):\n","    \"\"\"\n","    Rectified linear unit function.\n","    :param x: input\n","    \"\"\"\n","    \n","    #### [TODO 1] START CODE HERE #### \n","    x = np.maximum(0, x)\n","    #### END CODE HERE ####\n","    return x\n","\n","\n","def relu_grad(x):\n","    \"\"\"\n","    Compute gradient of ReLU.\n","    :param x: input\n","    \"\"\"\n","    \n","    #### [TODO 1] START CODE HERE #### \n","    da = (x > 0)*1\n","    #### END CODE HERE ####\n","    return da\n","\n","\n","def tanh(x):\n","    \"\"\"\n","    Tanh function.\n","    :param x: input\n","    \"\"\"\n","   \n","    #### [TODO 1] START CODE HERE #### \n","    x = np.tanh(x)\n","    #### END CODE HERE ####\n","    return x\n","\n","\n","def tanh_grad(x):\n","    \"\"\"\n","    Compute gradient for tanh.\n","    :param x: input\n","    \"\"\"\n","\n","    #### [TODO 1] START CODE HERE ####\n","    da = 1 - np.tanh(x)**2\n","    #### END CODE HERE ####\n","    return da\n","\n","def softmax(x):\n","    \"\"\"\n","    Stable softmax function.\n","    :param x: input\n","    \"\"\"\n","    #### [TODO 1] START CODE HERE ####\n","    \n","    x = np.exp(x - np.max(x, axis=1, keepdims = True))\n","    s = np.sum(x, axis=1, keepdims = True)\n","    probs = x / s\n","    \n","    #### END CODE HERE ####\n","    return probs "],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0c92wODTVK1E","colab_type":"text"},"source":["#### Kiểm tra lại lại các hàm activation đã cài đặt\n","\n","Bạn có thể kiểm tra cái hàm bạn đã cài đặt bằng đoạn code bên dưới."]},{"cell_type":"code","metadata":{"id":"Neen94acVK1E","colab_type":"code","outputId":"ef81a357-3698-46c8-9202-071ecc57f3f7","executionInfo":{"status":"ok","timestamp":1569296887061,"user_tz":-420,"elapsed":20868,"user":{"displayName":"Dang Nguyen Minh","photoUrl":"","userId":"06404449821748651554"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["import pickle\n","\n","np.random.seed(2019)\n","func_test = [\"sigmoid\", \"relu\", \"tanh\", \"sigmoid_grad\", \"relu_grad\", \"tanh_grad\", \"softmax\"]\n","test_activation = dict()\n","results = []\n","with open(\"test/activation.pkl\", \"rb\") as f:\n","    test_activation = pickle.load(f)\n","\n","test_x = test_activation[\"test_x\"]\n","sample = test_activation[\"sample\"]\n","\n","for i, func_str in enumerate(func_test):\n","    func = eval(func_str)\n","    if func(test_x) is None:\n","        results.append(func_str)\n","        continue\n","    sample.append(func(test_x))\n","    if not np.allclose(func(test_x), sample[i]):\n","        results.append(func_str)\n","\n","if len(results) == 0:\n","    print(\"Test PASS!\")\n","else:\n","    print(\"Test FAILED: \" + \", \".join(results))"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Test PASS!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"VZTlOP46VK1G","colab_type":"text"},"source":["### Class `HiddenLayer` \n","\n","#### Hướng dẫn:\n","\n","1. Hàm `forward`:\n","    - Hàm nhận vào tham số input $X$ (là output của hidden layer trước theo chiều forward, layer $l-1$).\n","    - Tính linear transformation của $X$ ($A^{[l-1]}$): $Z^{[l]} = XW$.\n","    - Sau đó tính nonlinear transformation: $A^{[l]} = \\sigma(Z^{[l]})$ với $\\sigma$ là hàm activation.\n","    \n","    \n","2. Hàm `backward`:\n","    - Hàm nhận vào 2 tham số input `X`(là output của hidden layer trước đó theo chiều forward) với `delta_prev` (delta trước đó theo chiều backward).\n","    - Tính delta tại layer $l$: \n","    \n","        $$\\delta^{[l]} = \\frac{\\partial J}{\\partial A^{[l]}}\\frac{\\partial A^{[l]}}{\\partial Z^{[l]}} =  \\delta^{[l+1]} * \\sigma'(Z^{[l]})$$ \n","\n","        Chú ý: $*$ operation là element-wise multiplication.\n","    - Tính W_grad (without regularization): $\\nabla W^{[l]} = (A^{[l-1]})^T\\delta^{[l]} $\n","    - With regularization:  $\\nabla W^{[l]} = (A^{[l-1]})^T\\delta^{[l]} + \\frac{\\lambda}{N} W^{[l]}$ với $\\lambda$ là hệ số regularization (hyperparameter mình sẽ chọn)."]},{"cell_type":"markdown","metadata":{"id":"qfSLDffVYQdc","colab_type":"text"},"source":["#### \\[TODO 2\\] Hàm `forward`\n","Định nghĩa hàm `forward` trong class `HiddenLayer` (1đ)"]},{"cell_type":"markdown","metadata":{"id":"d4uUr2FKh97J","colab_type":"text"},"source":["#### \\[TODO 3\\] Hàm `backward` \n","Định nghĩa hàm `backward` trong class `HiddenLayer` (2đ)\n","  + Tính W_grad (without regularization) (1đ)\n","  + Tính W_grad (with L2 regularization) (1đ)"]},{"cell_type":"code","metadata":{"id":"eROUmEreVK1H","colab_type":"code","colab":{}},"source":["class HiddenLayer:\n","    \"\"\"\n","    Abstract hidden layer used in Neural Network.\n","    \"\"\"\n","    \n","    def __init__(self, num_neurons, activation, reg = 0.0):\n","        \"\"\"\n","        Constructor for abstract hidden layer.\n","        \n","        Parameters\n","        ----------\n","        num_neurons: (integer) specify number of neurons in this layer.\n","        activation: (string) indicating which activation function to be used.  \n","                        the string must be in [\"sigmoid\", \"relu\", \"tanh\", \"softmax\"]\n","        reg: (float) regularization coefficient to help with overfitting.\n","        \"\"\"\n","        assert activation in [\"sigmoid\", \"relu\", \"tanh\", \"softmax\"], \"Activation must be in [sigmoid, relu, tanh, softmax]\"\n","        self.num_neurons = num_neurons\n","        self.W = None \n","        self.activation = activation\n","        self.reg = reg\n","\n","    def forward(self, X):\n","        \"\"\"\n","        Compute nonlinear function of input X.\n","            X -> LINEAR -> ACTIVATION.\n","            \n","        `activation_function` variable below is equal to this piece of code:\n","        \n","            if (self.activation == 'sigmoid'):\n","                activation_function = sigmoid\n","            elif (self.activation == 'relu'):\n","                activation_function = reLU\n","            elif (self.activation == 'tanh'):\n","                activation_function = tanh\n","            elif (self.activation == 'softmax'):\n","                activation_function = softmax\n","        \n","        Parameters\n","        ----------\n","        X: output of the previous layer (input for the current layer).\n","        \n","        Returns\n","        -------\n","        A: output of the current layer.\n","        \"\"\"\n","        if self.W is None:\n","            W_shape = (X.shape[1], self.num_neurons)\n","            self.W = np.random.normal(0, np.sqrt(2/(X.shape[1]+self.num_neurons)), W_shape)\n","        \n","        activation_function = eval(self.activation) # this returns function variable\n","        \n","        #### [TODO 2] START CODE HERE ####\n","        self.Z = X @ self.W\n","        A = activation_function(self.Z) # use `activation_function` function above apply to `Z`\n","        #### END CODE HERE ####\n","        return A\n","\n","    def backward(self, X, delta_prev):\n","        \"\"\"\n","        Compute gradient w.r.t X and W at the current layer.\n","            X <- LINEAR <- ACTIVATION.\n","            W <- LINEAR <- ACTIVATION.\n","        \n","        Parameters\n","        ----------\n","        X: output of the previous layer (input for the current layer).\n","        delta_prev: delta dot product with W computed from the next layer (in feedforward direction) \n","                                or previous layer (in backpropagation direction)\n","        \"\"\"\n","        activation_grad_function = eval(self.activation + \"_grad\")\n","        z = self.Z\n","\n","        #### [TODO 3] START CODE HERE ####                                \n","        delta = delta_prev * activation_grad_function(self.Z)\n","        W_grad = X.T @ delta # without regularization\n","        #### END CODE HERE ####\n","\n","        #### [TODO 3] START CODE HERE #### \n","        W_grad += self.reg * self.W / len(X) # with L2 regularization\n","        #### END CODE HERE ####\n","        \n","        return W_grad, delta"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LKWbE4ESVK1I","colab_type":"text"},"source":["#### Kiểm tra class `HiddenLayer` được cài đặt\n","\n","Sau khi hoàn thành TODO 2 và TODO 3, bạn có thể kiểm tra class `HiddenLayer` bạn đã cài đặt bằng cách chạy cell bên dưới:"]},{"cell_type":"code","metadata":{"id":"vUZJGVMuVK1J","colab_type":"code","outputId":"50e5909e-ef5b-460a-99a2-676c5383e370","executionInfo":{"status":"ok","timestamp":1569296887065,"user_tz":-420,"elapsed":20845,"user":{"displayName":"Dang Nguyen Minh","photoUrl":"","userId":"06404449821748651554"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["np.random.seed(2019)\n","case_1 = HiddenLayer(10, \"sigmoid\", reg=0)\n","case_2 = HiddenLayer(14, \"tanh\", reg=0)\n","case_3 = HiddenLayer(17, \"relu\", reg=0)\n","\n","case_mapping = {0: \"(foward)\", 1: \"(backward W_grad)\", 2: \"(backward delta)\"}\n","\n","with open(\"test/hidden.pkl\", \"rb\") as f:\n","    test_hidden = pickle.load(f)\n","\n","test_x = test_hidden[\"test_x\"]\n","sample = test_hidden[\"sample\"]\n","test_delta = test_hidden[\"test_delta\"]\n","results = []\n","for ind, cl_str in enumerate([\"case_1\", \"case_2\", \"case_3\"]):\n","    hidden_layer = eval(cl_str)\n","    case = (hidden_layer.forward(test_x), ) + hidden_layer.backward(test_x, test_delta[ind])\n","    for i in range(3):\n","        if not np.allclose(sample[ind][i], case[i]):\n","            results.append(cl_str + case_mapping[i])\n","\n","print(\"TEST HiddenLayer WITHOUT REGULARIZATION\")\n","if len(results) == 0:\n","    print(\"Test PASS!\")\n","else:\n","    print(\"Test FAILED: \" + \", \".join(results))"],"execution_count":7,"outputs":[{"output_type":"stream","text":["TEST HiddenLayer WITHOUT REGULARIZATION\n","Test PASS!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"t3490b6mfJwM","colab_type":"text"},"source":["Các bạn nên chắc chắn rằng mình đã pass hết các test cases để làm tiếp."]},{"cell_type":"markdown","metadata":{"id":"P8HSsFNGfMqi","colab_type":"text"},"source":["#### Kiểm tra Hidden Layer với Regularization\n","\n","Các bạn có thể bỏ qua phần test case này nếu chưa làm với Regularization."]},{"cell_type":"code","metadata":{"id":"dYsQBucQfQWq","colab_type":"code","outputId":"b5cd08a7-f07e-4e7f-ca36-49537d902ce6","executionInfo":{"status":"ok","timestamp":1569296887067,"user_tz":-420,"elapsed":20835,"user":{"displayName":"Dang Nguyen Minh","photoUrl":"","userId":"06404449821748651554"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["np.random.seed(2019)\n","case_1 = HiddenLayer(10, \"sigmoid\", reg=0.9)\n","case_2 = HiddenLayer(14, \"tanh\", reg=0.9)\n","case_3 = HiddenLayer(17, \"relu\", reg=0.9)\n","\n","case_mapping = {0: \"(foward)\", 1: \"(backward W_grad)\", 2: \"(backward delta)\"}\n","\n","with open(\"test/hidden_reg.pkl\", \"rb\") as f:\n","    test_hidden = pickle.load(f)\n","\n","test_x = test_hidden[\"test_x\"]\n","sample = test_hidden[\"sample\"]\n","test_delta = test_hidden[\"test_delta\"]\n","results = []\n","for ind, cl_str in enumerate([\"case_1\", \"case_2\", \"case_3\"]):\n","    hidden_layer = eval(cl_str)\n","    case = (hidden_layer.forward(test_x), ) + hidden_layer.backward(test_x, test_delta[ind])\n","    for i in range(3):\n","        if not np.allclose(sample[ind][i], case[i]):\n","            results.append(cl_str + case_mapping[i])\n","\n","print(\"TEST HiddenLayer WITH REGULARIZATION\")\n","if len(results) == 0:\n","    print(\"Test PASS!\")\n","else:\n","    print(\"Test FAILED: \" + \", \".join(results))"],"execution_count":8,"outputs":[{"output_type":"stream","text":["TEST HiddenLayer WITH REGULARIZATION\n","Test PASS!\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"LRqpw1yWVK1K","colab_type":"text"},"source":["### Class `NeuralNetwork`\n","\n","#### Hướng dẫn:\n","\n","1. Hàm `forward`:\n","    - Sử dụng hàm `forward` class `HiddenLayer`. Output của layer này [$l$] sẽ là input của layer sau [$l+1$]. Thêm output của layer [$l$] vào list `all_X` \n","\n","\n","2. Hàm `compute_loss`:\n","    - Cross-entropy, không regularization: $$J = -\\frac{1}{N}\\sum_{i=1}^N\\sum_{k=1}^C y_{ik}\\log(a_{ik}^{[L]})$$\n","    - Cross-entropy, L2 regularization: $$J = -\\frac{1}{N}\\sum_{i=1}^N\\sum_{k=1}^C y_{ik}\\log(a_{ik}^{[L]}) + \\frac{\\lambda}{2N}\\sum_{l=1}^L\\|\\mathbf{W}^{[l]} \\|_2^2$$\n","    \n","    \n","3. Hàm `backward`:\n","\n","    - Tính `delta_last` $\\delta^{[L]} $ và `grad_last` $\\nabla W^{[L]}$ theo hàm softmax (để ý superscript [L]): \n","    \n","        $$\\delta^{[L]} = \\frac{\\partial J}{\\partial \\mathbf{A}^{[L]}}\\frac{\\partial \\mathbf{A}^{[L]}}{\\partial \\mathbf{Z}^{[L]}} = \\frac{1}{N} (\\mathbf{A}^{[L]} - \\mathbf{Y})$$\n","        **- Without regularization**\n","        \n","        $$\\nabla W^{[L]} = \\frac{\\partial J}{\\partial \\mathbf{A}^{[L]}}\\frac{\\partial \\mathbf{A}^{[L]}}{\\partial \\mathbf{Z}^{[L]}}\\frac{\\partial \\mathbf{Z}^{[L]}}{\\partial \\mathbf{W}^{[L]}} = \\delta^{[L]} \\frac{\\partial \\mathbf{Z}^{[L]}}{\\partial \\mathbf{W}^{[L]}} = \\frac{1}{N} (\\mathbf{A}^{[L-1]})^T (\\mathbf{A}^{[L]} - \\mathbf{Y}) $$\n","        $$$$\n","        **- With L2 regularization**\n","        $$\\nabla \\mathbf{W}^{[L]} = (\\mathbf{A}^{[L-1]})^T \\delta^{[L]} + \\frac{\\lambda}{N} W^{[l]}$$\n","\n","    - Tính `delta_prev` $\\delta^{[l]}$ và `grad_W` $\\nabla \\mathbf{W}^{[l]}$ ở các tầng ở giữa:\n","    \n","        $$\\delta^{[l]} = \\left(\\delta^{[l+1]}(\\mathbf{W}^{[l+1]})^T\\right) * \\sigma'(\\mathbf{Z^{[l]}})$$\n","        **- Without regularization**\n","        \n","        $$\\nabla \\mathbf{W}^{[l]} = (\\mathbf{A}^{[l-1]})^T \\delta^{[l]}$$\n","        **- With L2 regularization**\n","        $$\\nabla \\mathbf{W}^{[l]} = (\\mathbf{A}^{[l-1]})^T \\delta^{[l]} + \\frac{\\lambda}{N} W^{[l]}$$\n","        \n","        Mục đích tính `delta` để tính gradient của `W` ở các tầng trước theo chiều forward."]},{"cell_type":"markdown","metadata":{"id":"9sjTUfDesi3b","colab_type":"text"},"source":["#### \\[TODO 4\\] Hàm `forward`\n","Định nghĩa hàm `forward` trong class `NeuralNetwork` (0.5đ)"]},{"cell_type":"markdown","metadata":{"id":"TP4gB_yyjPoN","colab_type":"text"},"source":["#### \\[TODO 5\\] Hàm `compute_loss`\n","Định nghĩa hàm `compute_loss` trong class `NeuralNetwork` (1.5đ)\n","  + Loss without regularization\n","  + Loss of L2 regularization"]},{"cell_type":"markdown","metadata":{"id":"07CXsA6GjTXY","colab_type":"text"},"source":["#### \\[TODO 6\\] Hàm `compute_delta_grad_last`\n","Định nghĩa hàm `compute_delta_grad_last` trong class `NeuralNetwork` (1đ)\n","  + Without regularization (0.5đ)\n","  + With L2 regularization (0.5đ)"]},{"cell_type":"markdown","metadata":{"id":"JJjOZsXWjqz2","colab_type":"text"},"source":["#### [TODO 7] Hàm `backward`\n","Định nghĩa hàm `backward` trong class `NeuralNetwork` (1đ)"]},{"cell_type":"markdown","metadata":{"id":"Xv9USNyilQys","colab_type":"text"},"source":["#### \\[TODO 8\\] Hàm `update_weight_momentum`\n","Định nghĩa hàm `update_weight_momentum` trong class `NeuralNetwork`. (1đ)"]},{"cell_type":"code","metadata":{"id":"IqaUfUyZVK1L","colab_type":"code","colab":{}},"source":["class NeuralNetwork:\n","    \n","    def __init__(self, learning_rate, num_class=2, reg = 1e-5):\n","        self.layers = []\n","        self.reg = reg\n","        self.num_class = num_class\n","        self.learning_rate = learning_rate\n","        \n","    def add_layer(self, num_neurons, activation):\n","        \"\"\"\n","        Function to add a hidden layer to neural network.\n","        \n","        Parameters\n","        ----------\n","        num_neurons: hyperparameter that specify nuber of neurons new hidden layer have/\n","        activation: string, indicating which activation function to be used\n","        \"\"\"\n","        assert activation in [\"sigmoid\", \"relu\", \"tanh\", \"softmax\"], \"Activation must be in [sigmoid, relu, tanh, softmax]\"\n","        self.layers.append(HiddenLayer(num_neurons, activation, self.reg))\n","\n","    def forward(self, X):\n","        \"\"\"\n","        Do forward propagation in the neural network\n","        \n","        Parameters\n","        ----------\n","        X: raw input X.\n","        \n","        Returns\n","        -------\n","        all_X: list of all X computed at each layer. \n","        \"\"\"\n","        all_X = [X]\n","        #### [TODO 4] START CODE HERE ####\n","        for i in range(len(self.layers)):\n","            all_X.append(self.layers[i].forward(all_X[-1]))\n","            \n","        #### END CODE HERE ####\n","        \n","        return all_X\n","    \n","    def compute_loss(self, Y, Y_hat):\n","        \"\"\"\n","        Compute the average cross entropy loss using Y (label) and Y_hat (predicted class)\n","                    and plus with regularization loss.\n","        Parameters\n","        ----------\n","        Y:  the label, the actual class of the samples. This is one-hot encoding vector.\n","                E.g: [0, 1, 2, 1] => [[1, 0, 0],\n","                                      [0, 1, 0],\n","                                      [0, 0, 1],\n","                                      [0, 1, 0]]\n","        Y_hat: the propabilities of classes (output of softmax).\n","        \n","        Returns\n","        -------\n","        \n","        \"\"\"\n","        #estimating cross entropy loss from y_hat and y\n","        #### [TODO 5] START CODE HERE ####\n","        \n","        correct_log_probs = -1/len(Y)*np.sum(Y*np.log(Y_hat))\n","        data_loss = correct_log_probs # loss without regularization\n","        \n","        #### END CODE HERE ####\n","\n","        #estimating regularization loss from all layers\n","        reg_loss = 0.0\n","        #### [TODO 5] START CODE HERE ####\n","        # compute reg_loss\n","        \n","        for i in range(len(self.layers)):\n","            reg_loss += np.linalg.norm(self.layers[i].W)**2\n","        reg_loss = self.reg*reg_loss/(2*len(Y))\n","        \n","        #### END CODE HERE ####\n","        data_loss += reg_loss # loss with L2 regularization\n","\n","        return data_loss\n","\n","    def compute_delta_grad_last(self, Y, all_X):\n","        \"\"\"\n","        Special formula to compute delta last and gradient last.\n","        \n","        Parameters\n","        ----------\n","        Y:  the label, the actual class of the samples. This is one-hot encoding vector.\n","                E.g: [0, 1, 2, 1] => [[1, 0, 0],\n","                                      [0, 1, 0],\n","                                      [0, 0, 1],\n","                                      [0, 1, 0]]\n","                                      \n","        all_X: raw input data and activation output from every layer\n","        \"\"\"\n","        n = Y.shape[0]\n","        #### [TODO 6] START CODE HERE ####\n","        delta_last = 1/len(Y)*(all_X[-1] - Y)\n","        grad_last = (all_X[-2]).T @ delta_last\n","        #### END CODE HERE ####\n","        return delta_last, grad_last\n","\n","    def backward(self, Y, all_X):\n","        \"\"\"\n","        Backpropagation algorithm to compute gradient at each layer.\n","\n","        Parameters\n","        ----------\n","        Y:  the label, the actual class of the samples. This is one-hot encoding vector.\n","                E.g: [0, 1, 2, 1] => [[1, 0, 0],\n","                                      [0, 1, 0],\n","                                      [0, 0, 1],\n","                                      [0, 1, 0]]\n","        all_X: raw input data and activation output from every layer\n","        \n","        Returns\n","        -------\n","        grad_list: list of gradients we've just computed at each layer.\n","        \"\"\"\n","        \n","        # Compute delta_last and  grad_last from the output\n","        delta_prev, grad_last = self.compute_delta_grad_last(Y, all_X)\n","\n","        grad_list = [grad_last]\n","\n","        for i in range(len(self.layers) - 1)[::-1]:\n","            prev_layer = self.layers[i+1] # previous layer as backward direction\n","            layer = self.layers[i]\n","            X = all_X[i]\n","            #### [TODO 7] START CODE HERE ####\n","            delta_prev = delta_prev @ prev_layer.W.T # dot product of delta_prev and W_prev\n","            grad_W, delta_prev = layer.backward(X, delta_prev)\n","            #### END CODE HERE ####\n","            grad_list.append(grad_W)\n","\n","        grad_list = grad_list[::-1]\n","        return grad_list\n","\n","    def update_weight(self, grad_list):\n","        \"\"\"\n","        Update W by gradient descent using the computed gradient.\n","        \n","        Parameters\n","        ----------\n","        grad_list: (list) list of gradients from all layers that computed from backward function above\n","        learning_rate: (float) learning rate for gradient descent.\n","        \"\"\"\n","        for i, layer in enumerate(self.layers):\n","            grad = grad_list[i]\n","            layer.W = layer.W - self.learning_rate * grad\n","    \n","    def update_weight_momentum(self, grad_list, momentum_rate):\n","        \"\"\"\n","        Update W using gradient descent with momentum\n","\n","        Parameters\n","        ----------\n","        grad_list: (list) list of gradients from all layers that computed from backward function above\n","        learning_rate: (float) learning rate.\n","        momentum_rate: (float) momentum rate.\n","        \"\"\"\n","        if not hasattr(self, \"momentum\"):\n","            self.momentum = [np.zeros_like(grad) for grad in grad_list]\n","            \n","        #### [TODO 8] START CODE HERE ####\n","        \n","        for i, layer in enumerate(self.layers):\n","            grad = grad_list[i]\n","            self.momentum = momentum_rate * self.momentum + self.learning_rate * grad\n","            layer.W = layer.W - self.momentum\n","            \n","        #### END CODE HERE ####\n","            \n","    def predict(self, X_test):\n","        Y_hat = self.forward(X_test)[-1]\n","        return np.argmax(Y_hat, axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Gz3F7s_sowbh","colab_type":"text"},"source":["### Training\n","\n","Sau khi định nghĩa các classes HiddenLayer và NeuralNetwork, chúng ta thực hiện huấn luyện (training) mô hình. Trong bài tập này, 2 kỹ thuật training được giới thiệu:\n","  \n","  + Batch training\n","  + Mini-batch training"]},{"cell_type":"markdown","metadata":{"id":"0FfLh29bo1Cd","colab_type":"text"},"source":["#### Batch train"]},{"cell_type":"markdown","metadata":{"id":"A0JxaImbVK1N","colab_type":"text"},"source":["Định nghĩa hàm `batch_train` để  train trên toàn dữ liệu. Có nghĩa là weights `W` trên mạng neural network sẽ được update trên toàn điểm dữ liệu, thay vì ở mỗi batch điểm dữ liệu như là `mini_batch_train`"]},{"cell_type":"code","metadata":{"id":"jFB3Ty-iVK1N","colab_type":"code","colab":{}},"source":[" def batch_train(X_train, Y_train, epochs, neural_network, bat=False):\n","    \"\"\"\n","    Using batch train.\n","    \n","    Parameters\n","    ----------\n","    X_train: training data X.\n","    Y_train: training data Y.\n","    epochs: number of iterations that we should use to train.\n","    neural_network: NeuralNetwork object instance above.\n","    \"\"\"\n","    all_loss = []\n","    display_step = 100 if bat else 10\n","    \n","    for e in range(epochs):\n","        all_X = neural_network.forward(X_train)\n","        loss = neural_network.compute_loss(Y_train, all_X[-1])\n","        grad_list = neural_network.backward(Y_train, all_X)\n","        neural_network.update_weight(grad_list)\n","        \n","        all_loss.append(loss)\n","        \n","        if (e+1) % display_step == 0:\n","            display.clear_output(wait=True)\n","            if bat:\n","                y_hat = neural_network.forward(X_train)[-1]\n","                visualize_point(X_train, np.argmax(Y_train,axis=1), y_hat)\n","            plot_loss(all_loss, title=\"Loss epoch %s: %.4f\" % (e+1, loss), color=2)\n","            plt.show()\n","            plt.pause(0.01)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"l3sb-LfCVK1P","colab_type":"text"},"source":["#### \\[TODO 9\\] Mini-batch train\n","Định nghĩa hàm `mini_batch_train` (2đ)\n","\n","**Pseudocode:**\n","\n","```\n","-> For each epoch do:\n","    -> Shuffle data\n","    -> Set initial loss at that epoch equal to 0\n","    -> Calculate number of batches based on batch size and total number of data points\n","    -> For each batch do:\n","        -> all_X := nn.forward() at that batch, Y_hat is equal all_X[-1]\n","        -> compute loss at that batch\n","        -> initial loss += computed loss at that batch\n","        -> grad_list := nn.backward()\n","        -> update weights.\n","```"]},{"cell_type":"code","metadata":{"id":"w_o23rbaVK1P","colab_type":"code","colab":{}},"source":[" def minibatch_train(X_train, Y_train, epochs, batch_size, num_class, neural_network):\n","    \"\"\"\n","    Using batch train.\n","    \n","    Parameters\n","    ----------\n","    X_train: training data X.\n","    Y_train: training data Y.\n","    epochs: number of iterations that we should use to train.\n","    batch_size: number of batch at each update.\n","    neural_network: NeuralNetwork object instance above.\n","    \n","    \"\"\"\n","    #### [TODO 9] START CODE HERE ####\n","    \n","    all_loss = []\n","    batches = len(X_train) // batch_size\n","    indexes = list(range(len(X_train)))\n","    for e in range(epochs):\n","        np.random.shuffle(indexes) # Shuffle data by shuffling indexes\n","        loss = 0\n","        for b in range(batches):\n","            X = X_train[b*batch_size:(b+1)*batch_size]\n","            Y = Y_train[b*batch_size:(b+1)*batch_size]\n","            all_X = neural_network.forward(X)\n","            loss += neural_network.compute_loss(Y, all_X[-1])\n","            grad_list = neural_network.backward(Y, all_X)\n","            neural_network.update_weight(grad_list)\n","            \n","        \n","        all_loss.append(loss)\n","        display.clear_output(wait=True)\n","        plot_loss(all_loss, title=\"Loss epoch %s: %.4f\" % (e+1, loss), color=2)\n","        plt.show()\n","        \n","    #### END CODE HERE ####"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b0DirH620RYD","colab_type":"text"},"source":["### Bat Classification"]},{"cell_type":"markdown","metadata":{"id":"bpnU_FBy1IId","colab_type":"text"},"source":["#### Hyperparameters"]},{"cell_type":"code","metadata":{"id":"X22sH6ug1LqZ","colab_type":"code","colab":{}},"source":["# Thay đổi giá trị của các hyperparameter bên dưới và\n","# quan sát sự thay đổi của loss và quá trình training \n","EPOCHS = 1000 #4000\n","LEARNING_RATE = 0.05 #0.01\n","REG= 1e-5\n","BATCH_SIZE = 64"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mUGCZeKq2M-I","colab_type":"text"},"source":["#### Định nghĩa hàm `bat_classification`"]},{"cell_type":"code","metadata":{"id":"7R_OW98uVK1R","colab_type":"code","colab":{}},"source":["def bat_classification(use_batch_train=True):\n","    # Load data from file\n","    # Make sure that bat.dat is in data/\n","    train_X, train_Y, test_X, test_Y = get_bat_data()\n","    train_X, _, test_X = normalize(train_X, train_X, test_X)    \n","\n","    test_Y  = test_Y.flatten()\n","    train_Y = train_Y.flatten()\n","    num_class = (np.unique(train_Y)).shape[0]\n","\n","    # Pad 1 as the third feature of train_x and test_x\n","    train_X = add_one(train_X) \n","    test_X = add_one(test_X)\n","    \n","    train_Y = create_one_hot(train_Y, num_class)\n","\n","    # Create NN classifier\n","    # Bạn có thể biến đổi thêm/bớt hidden layer, thay đổi hàm activation cho mỗi layer\n","    # và quan sát sự khác biệt trong quá trình train.\n","    net = NeuralNetwork(learning_rate=LEARNING_RATE, num_class=num_class, reg=REG)\n","    net.add_layer(100, 'relu')\n","    net.add_layer(100, 'relu')\n","    net.add_layer(100, 'relu')\n","    net.add_layer(num_class, 'softmax')\n","\n","    if use_batch_train:\n","        #Batch training - train all dataset\n","        batch_train(train_X, train_Y, EPOCHS, net)\n","    else:\n","        #Minibatch training - training dataset using Minibatch approach\n","        minibatch_train(train_X, train_Y, EPOCHS, BATCH_SIZE, num_class, net)\n","    metrics = confusion_matrix(test_Y, net.predict(test_X))\n","    print(\"Confusion metrix: \")\n","    print(metrics)\n","    \n","    print(\"Accuracy: \")\n","    print(metrics.trace()/test_Y.shape[0])"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"muLdYJbV2dlM","colab_type":"text"},"source":["#### Training"]},{"cell_type":"code","metadata":{"scrolled":true,"id":"4xn572XVVK1V","colab_type":"code","outputId":"979dbafc-40cd-4eed-ae6d-25ce5f37d973","executionInfo":{"status":"ok","timestamp":1569297251331,"user_tz":-420,"elapsed":26,"user":{"displayName":"Dang Nguyen Minh","photoUrl":"","userId":"06404449821748651554"}},"colab":{"base_uri":"https://localhost:8080/","height":383}},"source":["bat_classification(use_batch_train=False)"],"execution_count":14,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Confusion metrix: \n","[[1387  100    0]\n"," [  27 1006   75]\n"," [   0   13  392]]\n","Accuracy: \n","0.9283333333333333\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"dHRIZzum4Q7v","colab_type":"text"},"source":["### MNIST Classification"]},{"cell_type":"markdown","metadata":{"id":"TAv5PeHq4YRQ","colab_type":"text"},"source":["#### Hyperparameters"]},{"cell_type":"code","metadata":{"id":"89OE4gdG4bwy","colab_type":"code","colab":{}},"source":["# Thay đổi giá trị của các hyperparameter bên dưới và\n","# quan sát sự thay đổi của loss và quá trình training\n","EPOCHS = 300\n","LEARNING_RATE = 0.01\n","REG= 1e-5\n","BATCH_SIZE = 64"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"iFDjFonm5buD","colab_type":"text"},"source":["#### Định nghĩa hàm `mnist_classification`"]},{"cell_type":"code","metadata":{"id":"_EDiru6zVK1S","colab_type":"code","colab":{}},"source":["def mnist_classification(use_batch_train=True):\n","    # Load data from file\n","    # Make sure that fashion-mnist/*.gz is in data/\n","    train_X, train_Y, val_X, val_Y, test_X, test_Y = get_mnist_data(1)\n","    train_X, val_X, test_X = normalize(train_X, val_X, test_X)    \n","    \n","    num_class = (np.unique(train_Y)).shape[0]\n","\n","    # Pad 1 as the third feature of train_x and test_x\n","    train_X = add_one(train_X)\n","    val_X = add_one(val_X)\n","    test_X = add_one(test_X)\n","    \n","    train_Y = create_one_hot(train_Y, num_class)\n","    val_Y = create_one_hot(val_Y, num_class)\n","\n","    # Create NN classifier\n","    net = NeuralNetwork(learning_rate=LEARNING_RATE, num_class=num_class, reg=REG)\n","    net.add_layer(128, 'relu')\n","    net.add_layer(256, 'relu')\n","    net.add_layer(100, 'relu')\n","    net.add_layer(64, 'relu')\n","    net.add_layer(num_class, 'softmax')\n","    \n","    if use_batch_train:\n","        #Batch training - train all dataset\n","        batch_train(train_X, train_Y, EPOCHS, net)\n","    else:\n","        #Minibatch training - training dataset using Minibatch approach\n","        minibatch_train(train_X, train_Y, EPOCHS, BATCH_SIZE, num_class, net)\n","    metrics = confusion_matrix(test_Y, net.predict(test_X))\n","    print(\"Confusion metrix: \")\n","    print(metrics)\n","\n","    print(\"Accuracy: \")\n","    print(metrics.trace()/test_Y.shape[0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":false,"id":"M9dLdHlaVK1X","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":502},"outputId":"fd1f71e9-ec0b-4c6c-da4a-f3879728489f","executionInfo":{"status":"ok","timestamp":1569298202873,"user_tz":-420,"elapsed":43,"user":{"displayName":"Dang Nguyen Minh","photoUrl":"","userId":"06404449821748651554"}}},"source":["mnist_classification(use_batch_train=False)"],"execution_count":17,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAXoAAAEICAYAAABRSj9aAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRKTMKdGLiJS5/w+JExhNktIY/gAAAABJRU5ErkJggg==\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["Confusion metrix: \n","[[809   1  23  22   5   1 126   1  12   0]\n"," [ 10 966   1  18   2   0   2   0   1   0]\n"," [ 22   3 826   9  74   0  65   0   1   0]\n"," [ 29   7  14 886  33   0  24   1   6   0]\n"," [  1   2  93  30 812   0  57   0   5   0]\n"," [  2   0   1   1   0 956   1  26   1  12]\n"," [101   1  96  29  58   0 704   0  11   0]\n"," [  0   0   0   0   0  25   0 955   3  17]\n"," [  9   0   4   9   7   4   4   6 957   0]\n"," [  1   0   0   0   0  11   0  31   2 955]]\n","Accuracy: \n","0.8826\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"Pd7zbo_zVK1Y","colab_type":"text"},"source":["## II. Thực hiện Deep Neural Network với Tensorflow\n","\n","Phân loại tập dữ liệu Bat và MNIST với tensorflow."]},{"cell_type":"markdown","metadata":{"id":"m51lTqD6rW73","colab_type":"text"},"source":["### Import các thư viện cần thiết"]},{"cell_type":"code","metadata":{"id":"ozo-4la8VK1Z","colab_type":"code","colab":{}},"source":["import tensorflow as tf\n","\n","tf.enable_eager_execution()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"7N8tbrbxVK1b","colab_type":"code","colab":{}},"source":["tf.keras.backend.clear_session()"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ljzbayIbrmGw","colab_type":"text"},"source":["### Cài đặt class `DNNModel`\n","\n","Trong phần này bạn có các TODO sau:"]},{"cell_type":"markdown","metadata":{"id":"YJNmEejir4pM","colab_type":"text"},"source":["#### \\[TODO 10] Cài đặt kiến trúc mô hình\n","\n","Yêu cầu: xây dựng kiến trúc mạng với `tf.keras.Sequential()` để chồng các hidden layers và output layer với nhau. (1đ)"]},{"cell_type":"markdown","metadata":{"id":"fmKqmC97tttR","colab_type":"text"},"source":["#### [TODO 11] Cài đặt hàm tính accuracy\n","\n","Cài đặt hàm tính accuracy. (1đ)"]},{"cell_type":"markdown","metadata":{"id":"aDJZnuTEvgzH","colab_type":"text"},"source":["#### \\[TODO 12\\] Cài đặt hàm train\n","Cài đặt các bước để train mô hình trong class `DNNModel`. (2đ)"]},{"cell_type":"code","metadata":{"id":"gO7GgBTzVK1c","colab_type":"code","colab":{}},"source":["class DNNModel:\n","    def __init__(self, hidden_layers, num_classes, activation, epochs, optimizer):\n","        self.hidden_layers = hidden_layers\n","        self.num_classes = num_classes\n","        self.activation = activation\n","        self.epochs = epochs\n","        self.optimizer = optimizer\n","        \n","        #Hidden layers and output layers are stacked to form a model\n","        self.model = tf.keras.Sequential()\n","        #### [TODO 10] START CODE HERE ####\n","        #Add hidden layers\n","        for i in range(len(hidden_layers)):\n","            self.model.add(tf.keras.layers.Dense(hidden_layers[i], activation=self.activation))\n","        #Add output layer\n","        self.model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))\n","        self.model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n","        #### END CODE HERE ####\n","\n","    def loss(self, y_hat, y):\n","        \"\"\"\n","        Compute loss function.\n","        \n","        Parameters\n","        ----------\n","        y_hat: output of the last layer (softmax layer).\n","        y: labels/targets in our data.\n","        \n","        Returns\n","        -------\n","        Loss w.r.t y_hat and y. Should be a scalar.\n","        \"\"\"\n","        return tf.reduce_mean(-tf.reduce_sum(y*tf.log(y_hat), axis=1), axis=0)\n","\n","    def accuracy(self, y_hat, y):\n","        \"\"\"\n","        Compute accuracy score.\n","        \n","        Parameters\n","        ----------\n","        y_hat: output of the last layer (softmax layer).\n","        y: labels/targets in our data.\n","        \n","        Returns\n","        -------\n","        Accuracy w.r.t y_hat and y. Should be a scalar.\n","        \n","        \"\"\"\n","        #### [TODO 11] START CODE HERE ####\n","        \n","        acc = np.mean(y == y_hat)\n","        \n","        #### END CODE HERE ####\n","        return acc\n","\n","    def train(self, x_train, y_train):\n","        all_loss = []\n","        all_acc = []\n","        for e in range(self.epochs):\n","            #### [TODO 12] START CODE HERE ####\n","            \n","            result = self.model.fit(x_train, y_train).history\n","            loss = result['loss'][0]\n","            accuracy = result['acc'][0]\n","            all_loss.append(loss)\n","            all_acc.append(accuracy)\n","            \n","            #### END CODE HERE ####\n","            \n","            print(f\"\\rEpoch: {e}... Training loss: {loss}... Accuracy: {accuracy}\")\n","\n","    def predict(self, inputs):\n","        Y_hat = self.model(inputs)\n","        return tf.argmax(Y_hat, axis=1)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1H7YE6M4uccG","colab_type":"text"},"source":["### Bat classification"]},{"cell_type":"code","metadata":{"id":"ZOEopv9gVK1f","colab_type":"code","colab":{}},"source":["def tf_bat_classification():\n","     # Load data from file\n","    # Make sure that bat.dat is in data/\n","    train_X, train_Y, test_X, test_Y = get_bat_data()\n","    train_X, _, test_X = normalize(train_X, train_X, test_X)    \n","\n","    test_Y  = test_Y.flatten()\n","    train_Y = train_Y.flatten()\n","    num_class = (np.unique(train_Y)).shape[0]\n","\n","    # Pad 1 as the third feature of train_x and test_x\n","    train_X = add_one(train_X) \n","    test_X = add_one(test_X)\n","    \n","    train_Y = create_one_hot(train_Y, num_class)\n","    \n","    # DNN parameters\n","    hidden_layers = [100, 100, 100]\n","    learning_rate = 0.001\n","    epochs = 200\n","    activation = tf.nn.relu\n","    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","\n","    #Initialize model\n","    dnn = DNNModel(hidden_layers=hidden_layers, num_classes=num_class,\n","                   activation=activation, epochs=epochs, optimizer=optimizer)\n","    #Train\n","    dnn.train(train_X, train_Y)\n","\n","    # TEST\n","    # Confusion matrix\n","    metrics = confusion_matrix(test_Y, dnn.predict(test_X))\n","    print('Confusion matrix:')\n","    print(metrics)\n","    \n","    print(\"Accuracy: \")\n","    print(metrics.trace()/test_Y.shape[0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"SLs06JB9VK1i","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"bb065b49-ad5c-4eff-f5a1-46c120314d36","executionInfo":{"status":"ok","timestamp":1569298350466,"user_tz":-420,"elapsed":1484120,"user":{"displayName":"Dang Nguyen Minh","photoUrl":"","userId":"06404449821748651554"}}},"source":["tf_bat_classification()"],"execution_count":22,"outputs":[{"output_type":"stream","text":["Reading bat data...\n","EOF Reached\n","Done reading\n","WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","12000/12000 [==============================] - 1s 82us/sample - loss: 0.8468 - acc: 0.6058\n","Epoch: 0... Training loss: 0.8467992862860362... Accuracy: 0.6057500243186951\n","12000/12000 [==============================] - 1s 63us/sample - loss: 0.4687 - acc: 0.7887\n","Epoch: 1... Training loss: 0.4687263959248861... Accuracy: 0.7887499928474426\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.3885 - acc: 0.8253\n","Epoch: 2... Training loss: 0.38847252281506855... Accuracy: 0.8253333568572998\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.3268 - acc: 0.8637\n","Epoch: 3... Training loss: 0.32683642268180846... Accuracy: 0.8636666536331177\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.3143 - acc: 0.8593\n","Epoch: 4... Training loss: 0.31426074771086376... Accuracy: 0.859333336353302\n","12000/12000 [==============================] - 1s 62us/sample - loss: 0.2841 - acc: 0.8758\n","Epoch: 5... Training loss: 0.28414948654174804... Accuracy: 0.8757500052452087\n","12000/12000 [==============================] - 1s 63us/sample - loss: 0.2725 - acc: 0.8820\n","Epoch: 6... Training loss: 0.2725136370460192... Accuracy: 0.8820000290870667\n","12000/12000 [==============================] - 1s 63us/sample - loss: 0.2599 - acc: 0.8840\n","Epoch: 7... Training loss: 0.25986386416355767... Accuracy: 0.8840000033378601\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.2415 - acc: 0.8950\n","Epoch: 8... Training loss: 0.2415350396434466... Accuracy: 0.8949999809265137\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.2349 - acc: 0.8965\n","Epoch: 9... Training loss: 0.23489209417502085... Accuracy: 0.8964999914169312\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.2291 - acc: 0.9013\n","Epoch: 10... Training loss: 0.22913600649436314... Accuracy: 0.9012500047683716\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.2240 - acc: 0.9036\n","Epoch: 11... Training loss: 0.22402025113503138... Accuracy: 0.9035833477973938\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.2158 - acc: 0.9074\n","Epoch: 12... Training loss: 0.21576868830124538... Accuracy: 0.9074166417121887\n","12000/12000 [==============================] - 1s 58us/sample - loss: 0.2052 - acc: 0.9112\n","Epoch: 13... Training loss: 0.20520995863278707... Accuracy: 0.9111666679382324\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.2001 - acc: 0.9150\n","Epoch: 14... Training loss: 0.2000561145742734... Accuracy: 0.9150000214576721\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.2040 - acc: 0.9159\n","Epoch: 15... Training loss: 0.20402022549510002... Accuracy: 0.9159166812896729\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1792 - acc: 0.9250\n","Epoch: 16... Training loss: 0.1792109809319178... Accuracy: 0.925000011920929\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1878 - acc: 0.9194\n","Epoch: 17... Training loss: 0.18776594329873722... Accuracy: 0.9194166660308838\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1904 - acc: 0.9173\n","Epoch: 18... Training loss: 0.19038124967614808... Accuracy: 0.9173333048820496\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1863 - acc: 0.9209\n","Epoch: 19... Training loss: 0.18629754927754402... Accuracy: 0.9209166765213013\n","12000/12000 [==============================] - 1s 58us/sample - loss: 0.1810 - acc: 0.9236\n","Epoch: 20... Training loss: 0.18103950615723927... Accuracy: 0.9235833287239075\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1841 - acc: 0.9164\n","Epoch: 21... Training loss: 0.18406545053919157... Accuracy: 0.9164166450500488\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1743 - acc: 0.9234\n","Epoch: 22... Training loss: 0.17426546480258306... Accuracy: 0.9234166741371155\n","12000/12000 [==============================] - 1s 58us/sample - loss: 0.1873 - acc: 0.9175\n","Epoch: 23... Training loss: 0.1872576604485512... Accuracy: 0.9175000190734863\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1809 - acc: 0.9216\n","Epoch: 24... Training loss: 0.1809106156428655... Accuracy: 0.921583354473114\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1832 - acc: 0.9207\n","Epoch: 25... Training loss: 0.18323077914118766... Accuracy: 0.9206666946411133\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1801 - acc: 0.9194\n","Epoch: 26... Training loss: 0.1800874857008457... Accuracy: 0.9194166660308838\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1762 - acc: 0.9229\n","Epoch: 27... Training loss: 0.1761525276104609... Accuracy: 0.9229166507720947\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1864 - acc: 0.9187\n","Epoch: 28... Training loss: 0.18639533898234367... Accuracy: 0.918666660785675\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1717 - acc: 0.9249\n","Epoch: 29... Training loss: 0.1717349798430999... Accuracy: 0.924916684627533\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1684 - acc: 0.9267\n","Epoch: 30... Training loss: 0.16844479298591614... Accuracy: 0.9266666769981384\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1688 - acc: 0.9283\n","Epoch: 31... Training loss: 0.1687795989960432... Accuracy: 0.9283333420753479\n","12000/12000 [==============================] - 1s 62us/sample - loss: 0.1663 - acc: 0.9287\n","Epoch: 32... Training loss: 0.16630844485759735... Accuracy: 0.9286666512489319\n","12000/12000 [==============================] - 1s 63us/sample - loss: 0.1704 - acc: 0.9255\n","Epoch: 33... Training loss: 0.17036874787012737... Accuracy: 0.9254999756813049\n","12000/12000 [==============================] - 1s 62us/sample - loss: 0.1670 - acc: 0.9276\n","Epoch: 34... Training loss: 0.16697815601030985... Accuracy: 0.9275833368301392\n","12000/12000 [==============================] - 1s 63us/sample - loss: 0.1740 - acc: 0.9256\n","Epoch: 35... Training loss: 0.17397364141543706... Accuracy: 0.9255833625793457\n","12000/12000 [==============================] - 1s 63us/sample - loss: 0.1636 - acc: 0.9287\n","Epoch: 36... Training loss: 0.1635550661732753... Accuracy: 0.9286666512489319\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1669 - acc: 0.9268\n","Epoch: 37... Training loss: 0.1669372069935004... Accuracy: 0.9267500042915344\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1679 - acc: 0.9262\n","Epoch: 38... Training loss: 0.1678972431222598... Accuracy: 0.9262499809265137\n","12000/12000 [==============================] - 1s 62us/sample - loss: 0.1812 - acc: 0.9220\n","Epoch: 39... Training loss: 0.18121817383666833... Accuracy: 0.921999990940094\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1701 - acc: 0.9255\n","Epoch: 40... Training loss: 0.17009232680996259... Accuracy: 0.9254999756813049\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1611 - acc: 0.9276\n","Epoch: 41... Training loss: 0.16107426781455675... Accuracy: 0.9275833368301392\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1560 - acc: 0.9333\n","Epoch: 42... Training loss: 0.15604902355372904... Accuracy: 0.9332500100135803\n","12000/12000 [==============================] - 1s 62us/sample - loss: 0.1600 - acc: 0.9311\n","Epoch: 43... Training loss: 0.16002833125491936... Accuracy: 0.9310833215713501\n","12000/12000 [==============================] - 1s 63us/sample - loss: 0.1549 - acc: 0.9330\n","Epoch: 44... Training loss: 0.1549418066442013... Accuracy: 0.9330000281333923\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1625 - acc: 0.9276\n","Epoch: 45... Training loss: 0.16254748448729514... Accuracy: 0.9275833368301392\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1550 - acc: 0.9326\n","Epoch: 46... Training loss: 0.15499344724416733... Accuracy: 0.9325833320617676\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1658 - acc: 0.9293\n","Epoch: 47... Training loss: 0.1657832511390249... Accuracy: 0.9293333292007446\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1596 - acc: 0.9312\n","Epoch: 48... Training loss: 0.15960572351515293... Accuracy: 0.9311666488647461\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1572 - acc: 0.9305\n","Epoch: 49... Training loss: 0.15716464435557526... Accuracy: 0.9304999709129333\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1511 - acc: 0.9337\n","Epoch: 50... Training loss: 0.15113258131345114... Accuracy: 0.9337499737739563\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1716 - acc: 0.9233\n","Epoch: 51... Training loss: 0.17164774103959402... Accuracy: 0.9232500195503235\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1708 - acc: 0.9283\n","Epoch: 52... Training loss: 0.17080979027847448... Accuracy: 0.9283333420753479\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1617 - acc: 0.9297\n","Epoch: 53... Training loss: 0.16166704687972863... Accuracy: 0.9296666383743286\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1619 - acc: 0.9288\n","Epoch: 54... Training loss: 0.16189747077723343... Accuracy: 0.9288333058357239\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1697 - acc: 0.9275\n","Epoch: 55... Training loss: 0.16971468595167002... Accuracy: 0.9275000095367432\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1579 - acc: 0.9322\n","Epoch: 56... Training loss: 0.1578706793685754... Accuracy: 0.9321666955947876\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1512 - acc: 0.9338\n","Epoch: 57... Training loss: 0.1511911961187919... Accuracy: 0.9338333606719971\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1632 - acc: 0.9285\n","Epoch: 58... Training loss: 0.16321535447736582... Accuracy: 0.9284999966621399\n","12000/12000 [==============================] - 1s 58us/sample - loss: 0.1546 - acc: 0.9344\n","Epoch: 59... Training loss: 0.15459846732517082... Accuracy: 0.934416651725769\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1534 - acc: 0.9358\n","Epoch: 60... Training loss: 0.15337112492819627... Accuracy: 0.9358333349227905\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1580 - acc: 0.9302\n","Epoch: 61... Training loss: 0.15803083207209906... Accuracy: 0.9301666617393494\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1499 - acc: 0.9354\n","Epoch: 62... Training loss: 0.14985195123652617... Accuracy: 0.9354166388511658\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1627 - acc: 0.9268\n","Epoch: 63... Training loss: 0.16272849880407253... Accuracy: 0.9268333315849304\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1477 - acc: 0.9374\n","Epoch: 64... Training loss: 0.14774956327180067... Accuracy: 0.937416672706604\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1531 - acc: 0.9327\n","Epoch: 65... Training loss: 0.15306670269866784... Accuracy: 0.9326666593551636\n","12000/12000 [==============================] - 1s 58us/sample - loss: 0.1526 - acc: 0.9338\n","Epoch: 66... Training loss: 0.15261774873236816... Accuracy: 0.9338333606719971\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1507 - acc: 0.9341\n","Epoch: 67... Training loss: 0.15069241372992595... Accuracy: 0.9340833425521851\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1533 - acc: 0.9333\n","Epoch: 68... Training loss: 0.15328652967264256... Accuracy: 0.9332500100135803\n","12000/12000 [==============================] - 1s 58us/sample - loss: 0.1498 - acc: 0.9344\n","Epoch: 69... Training loss: 0.1498457801192999... Accuracy: 0.934416651725769\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1545 - acc: 0.9319\n","Epoch: 70... Training loss: 0.1545227321907878... Accuracy: 0.9319166541099548\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1505 - acc: 0.9340\n","Epoch: 71... Training loss: 0.1504566590289275... Accuracy: 0.9340000152587891\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1546 - acc: 0.9339\n","Epoch: 72... Training loss: 0.15464810932675999... Accuracy: 0.9339166879653931\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1429 - acc: 0.9367\n","Epoch: 73... Training loss: 0.1429461653282245... Accuracy: 0.9366666674613953\n","12000/12000 [==============================] - 1s 65us/sample - loss: 0.1531 - acc: 0.9327\n","Epoch: 74... Training loss: 0.15309315922111272... Accuracy: 0.9327499866485596\n","12000/12000 [==============================] - 1s 64us/sample - loss: 0.1486 - acc: 0.9371\n","Epoch: 75... Training loss: 0.14857395343730848... Accuracy: 0.9370833039283752\n","12000/12000 [==============================] - 1s 64us/sample - loss: 0.1448 - acc: 0.9394\n","Epoch: 76... Training loss: 0.14478818123787643... Accuracy: 0.9394166469573975\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1424 - acc: 0.9386\n","Epoch: 77... Training loss: 0.14235167525708675... Accuracy: 0.9385833144187927\n","12000/12000 [==============================] - 1s 62us/sample - loss: 0.1589 - acc: 0.9293\n","Epoch: 78... Training loss: 0.1588594765663147... Accuracy: 0.9292500019073486\n","12000/12000 [==============================] - 1s 63us/sample - loss: 0.1571 - acc: 0.9288\n","Epoch: 79... Training loss: 0.1571030604292949... Accuracy: 0.9288333058357239\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1550 - acc: 0.9313\n","Epoch: 80... Training loss: 0.15498097758119306... Accuracy: 0.9313333630561829\n","12000/12000 [==============================] - 1s 62us/sample - loss: 0.1474 - acc: 0.9363\n","Epoch: 81... Training loss: 0.14742925191422304... Accuracy: 0.9363333582878113\n","12000/12000 [==============================] - 1s 62us/sample - loss: 0.1481 - acc: 0.9367\n","Epoch: 82... Training loss: 0.14808338265617688... Accuracy: 0.9366666674613953\n","12000/12000 [==============================] - 1s 66us/sample - loss: 0.1393 - acc: 0.9389\n","Epoch: 83... Training loss: 0.1392558471163114... Accuracy: 0.9389166831970215\n","12000/12000 [==============================] - 1s 63us/sample - loss: 0.1495 - acc: 0.9361\n","Epoch: 84... Training loss: 0.1494605080038309... Accuracy: 0.9360833168029785\n","12000/12000 [==============================] - 1s 63us/sample - loss: 0.1428 - acc: 0.9410\n","Epoch: 85... Training loss: 0.14278647078573703... Accuracy: 0.9409999847412109\n","12000/12000 [==============================] - 1s 62us/sample - loss: 0.1435 - acc: 0.9378\n","Epoch: 86... Training loss: 0.1435146420399348... Accuracy: 0.937833309173584\n","12000/12000 [==============================] - 1s 62us/sample - loss: 0.1596 - acc: 0.9280\n","Epoch: 87... Training loss: 0.15964159241318704... Accuracy: 0.9279999732971191\n","12000/12000 [==============================] - 1s 58us/sample - loss: 0.1518 - acc: 0.9354\n","Epoch: 88... Training loss: 0.1517750536898772... Accuracy: 0.9354166388511658\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1529 - acc: 0.9318\n","Epoch: 89... Training loss: 0.152949631554385... Accuracy: 0.9318333268165588\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1452 - acc: 0.9388\n","Epoch: 90... Training loss: 0.14521726907541355... Accuracy: 0.9387500286102295\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1519 - acc: 0.9333\n","Epoch: 91... Training loss: 0.15192157270510992... Accuracy: 0.9332500100135803\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1407 - acc: 0.9401\n","Epoch: 92... Training loss: 0.1406854409823815... Accuracy: 0.9400833249092102\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1476 - acc: 0.9349\n","Epoch: 93... Training loss: 0.14763250913719336... Accuracy: 0.9349166750907898\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1421 - acc: 0.9388\n","Epoch: 94... Training loss: 0.14212212784339984... Accuracy: 0.9387500286102295\n","12000/12000 [==============================] - 1s 57us/sample - loss: 0.1569 - acc: 0.9330\n","Epoch: 95... Training loss: 0.15689315973718962... Accuracy: 0.9330000281333923\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1451 - acc: 0.9357\n","Epoch: 96... Training loss: 0.14508524971206982... Accuracy: 0.9356666803359985\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1416 - acc: 0.9363\n","Epoch: 97... Training loss: 0.14162467989822228... Accuracy: 0.9363333582878113\n","12000/12000 [==============================] - 1s 58us/sample - loss: 0.1412 - acc: 0.9381\n","Epoch: 98... Training loss: 0.14116511450211208... Accuracy: 0.9380833506584167\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1443 - acc: 0.9375\n","Epoch: 99... Training loss: 0.14434608837465446... Accuracy: 0.9375\n","12000/12000 [==============================] - 1s 62us/sample - loss: 0.1392 - acc: 0.9390\n","Epoch: 100... Training loss: 0.13924239076549808... Accuracy: 0.9390000104904175\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1450 - acc: 0.9362\n","Epoch: 101... Training loss: 0.14498339922726156... Accuracy: 0.9362499713897705\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1461 - acc: 0.9389\n","Epoch: 102... Training loss: 0.14610841397444407... Accuracy: 0.9389166831970215\n","12000/12000 [==============================] - 1s 63us/sample - loss: 0.1371 - acc: 0.9398\n","Epoch: 103... Training loss: 0.13706424741943676... Accuracy: 0.9397500157356262\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1444 - acc: 0.9355\n","Epoch: 104... Training loss: 0.1444494811197122... Accuracy: 0.9355000257492065\n","12000/12000 [==============================] - 1s 58us/sample - loss: 0.1417 - acc: 0.9388\n","Epoch: 105... Training loss: 0.14173182153950134... Accuracy: 0.9388333559036255\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1455 - acc: 0.9375\n","Epoch: 106... Training loss: 0.14549830117324988... Accuracy: 0.9375\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1456 - acc: 0.9373\n","Epoch: 107... Training loss: 0.14562218009432157... Accuracy: 0.937333345413208\n","12000/12000 [==============================] - 1s 58us/sample - loss: 0.1479 - acc: 0.9337\n","Epoch: 108... Training loss: 0.14792834016680717... Accuracy: 0.9336666464805603\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1424 - acc: 0.9367\n","Epoch: 109... Training loss: 0.14236897890766462... Accuracy: 0.9366666674613953\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1391 - acc: 0.9385\n","Epoch: 110... Training loss: 0.13907041921218236... Accuracy: 0.9384999871253967\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1505 - acc: 0.9358\n","Epoch: 111... Training loss: 0.1505176849712928... Accuracy: 0.9358333349227905\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1479 - acc: 0.9366\n","Epoch: 112... Training loss: 0.14792884821941454... Accuracy: 0.9365833401679993\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1365 - acc: 0.9391\n","Epoch: 113... Training loss: 0.1364789587383469... Accuracy: 0.9390833377838135\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1430 - acc: 0.9380\n","Epoch: 114... Training loss: 0.14304790849486987... Accuracy: 0.9380000233650208\n","12000/12000 [==============================] - 1s 58us/sample - loss: 0.1303 - acc: 0.9414\n","Epoch: 115... Training loss: 0.13029137601455051... Accuracy: 0.9414166808128357\n","12000/12000 [==============================] - 1s 58us/sample - loss: 0.1484 - acc: 0.9349\n","Epoch: 116... Training loss: 0.14836805881063145... Accuracy: 0.9349166750907898\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1374 - acc: 0.9411\n","Epoch: 117... Training loss: 0.13743915745119253... Accuracy: 0.9410833120346069\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1383 - acc: 0.9395\n","Epoch: 118... Training loss: 0.13830561056733132... Accuracy: 0.9394999742507935\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1371 - acc: 0.9407\n","Epoch: 119... Training loss: 0.13711589044580857... Accuracy: 0.940666675567627\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1475 - acc: 0.9365\n","Epoch: 120... Training loss: 0.14749905983358622... Accuracy: 0.9365000128746033\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1387 - acc: 0.9390\n","Epoch: 121... Training loss: 0.13872764247159164... Accuracy: 0.9390000104904175\n","12000/12000 [==============================] - 1s 58us/sample - loss: 0.1368 - acc: 0.9420\n","Epoch: 122... Training loss: 0.1368260909120242... Accuracy: 0.9419999718666077\n","12000/12000 [==============================] - 1s 58us/sample - loss: 0.1460 - acc: 0.9355\n","Epoch: 123... Training loss: 0.14599329079439244... Accuracy: 0.9355000257492065\n","12000/12000 [==============================] - 1s 62us/sample - loss: 0.1403 - acc: 0.9388\n","Epoch: 124... Training loss: 0.14030881093939146... Accuracy: 0.9388333559036255\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1372 - acc: 0.9409\n","Epoch: 125... Training loss: 0.13715024107694626... Accuracy: 0.9409166574478149\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1509 - acc: 0.9353\n","Epoch: 126... Training loss: 0.15091183876494565... Accuracy: 0.9353333115577698\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1401 - acc: 0.9383\n","Epoch: 127... Training loss: 0.14012999699264764... Accuracy: 0.9382500052452087\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1313 - acc: 0.9444\n","Epoch: 128... Training loss: 0.131263673538963... Accuracy: 0.9444166421890259\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1419 - acc: 0.9392\n","Epoch: 129... Training loss: 0.1419211773723364... Accuracy: 0.9392499923706055\n","12000/12000 [==============================] - 1s 58us/sample - loss: 0.1280 - acc: 0.9431\n","Epoch: 130... Training loss: 0.12801552981883288... Accuracy: 0.9430833458900452\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1432 - acc: 0.9367\n","Epoch: 131... Training loss: 0.14321358588337899... Accuracy: 0.9366666674613953\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1359 - acc: 0.9409\n","Epoch: 132... Training loss: 0.1358921486486991... Accuracy: 0.9409166574478149\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1344 - acc: 0.9407\n","Epoch: 133... Training loss: 0.1344004842241605... Accuracy: 0.940666675567627\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1484 - acc: 0.9369\n","Epoch: 134... Training loss: 0.14839992043375969... Accuracy: 0.9369166493415833\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1403 - acc: 0.9380\n","Epoch: 135... Training loss: 0.14026169280707837... Accuracy: 0.9380000233650208\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1350 - acc: 0.9402\n","Epoch: 136... Training loss: 0.13496575863162677... Accuracy: 0.9401666522026062\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1384 - acc: 0.9417\n","Epoch: 137... Training loss: 0.13840838224689167... Accuracy: 0.9416666626930237\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1410 - acc: 0.9390\n","Epoch: 138... Training loss: 0.1410268478045861... Accuracy: 0.9390000104904175\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1329 - acc: 0.9423\n","Epoch: 139... Training loss: 0.13293658422927063... Accuracy: 0.9423333406448364\n","12000/12000 [==============================] - 1s 62us/sample - loss: 0.1440 - acc: 0.9362\n","Epoch: 140... Training loss: 0.14396633135775724... Accuracy: 0.9361666440963745\n","12000/12000 [==============================] - 1s 62us/sample - loss: 0.1319 - acc: 0.9433\n","Epoch: 141... Training loss: 0.13186161626378695... Accuracy: 0.9433333277702332\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1343 - acc: 0.9397\n","Epoch: 142... Training loss: 0.13428922087450823... Accuracy: 0.9396666884422302\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1391 - acc: 0.9395\n","Epoch: 143... Training loss: 0.1391124268795053... Accuracy: 0.9394999742507935\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1385 - acc: 0.9398\n","Epoch: 144... Training loss: 0.13852895719806355... Accuracy: 0.9398333430290222\n","12000/12000 [==============================] - 1s 58us/sample - loss: 0.1403 - acc: 0.9401\n","Epoch: 145... Training loss: 0.14025080417096614... Accuracy: 0.9400833249092102\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1304 - acc: 0.9422\n","Epoch: 146... Training loss: 0.13040155183772245... Accuracy: 0.9421666860580444\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1337 - acc: 0.9426\n","Epoch: 147... Training loss: 0.13367535760253668... Accuracy: 0.9425833225250244\n","12000/12000 [==============================] - 1s 62us/sample - loss: 0.1337 - acc: 0.9430\n","Epoch: 148... Training loss: 0.13370649252335232... Accuracy: 0.9430000185966492\n","12000/12000 [==============================] - 1s 63us/sample - loss: 0.1397 - acc: 0.9392\n","Epoch: 149... Training loss: 0.13966622696816922... Accuracy: 0.9392499923706055\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1429 - acc: 0.9388\n","Epoch: 150... Training loss: 0.14291300550103186... Accuracy: 0.9388333559036255\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1358 - acc: 0.9420\n","Epoch: 151... Training loss: 0.13575849979619184... Accuracy: 0.9419999718666077\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1287 - acc: 0.9433\n","Epoch: 152... Training loss: 0.12865592162062725... Accuracy: 0.9433333277702332\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1462 - acc: 0.9370\n","Epoch: 153... Training loss: 0.1461946235646804... Accuracy: 0.9369999766349792\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1390 - acc: 0.9382\n","Epoch: 154... Training loss: 0.13904663137346507... Accuracy: 0.9381666779518127\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1350 - acc: 0.9410\n","Epoch: 155... Training loss: 0.1349955963070194... Accuracy: 0.9409999847412109\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1386 - acc: 0.9388\n","Epoch: 156... Training loss: 0.1385867995619774... Accuracy: 0.9388333559036255\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1314 - acc: 0.9433\n","Epoch: 157... Training loss: 0.1313663630709052... Accuracy: 0.9433333277702332\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1278 - acc: 0.9444\n","Epoch: 158... Training loss: 0.1278027917991082... Accuracy: 0.9444166421890259\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1375 - acc: 0.9379\n","Epoch: 159... Training loss: 0.13754362800717354... Accuracy: 0.9379166960716248\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1494 - acc: 0.9366\n","Epoch: 160... Training loss: 0.14943907334903875... Accuracy: 0.9365833401679993\n","12000/12000 [==============================] - 1s 62us/sample - loss: 0.1369 - acc: 0.9398\n","Epoch: 161... Training loss: 0.13690439322094122... Accuracy: 0.9397500157356262\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1303 - acc: 0.9442\n","Epoch: 162... Training loss: 0.13026332112650077... Accuracy: 0.9441666603088379\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1364 - acc: 0.9404\n","Epoch: 163... Training loss: 0.1364182870388031... Accuracy: 0.940416693687439\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1412 - acc: 0.9388\n","Epoch: 164... Training loss: 0.14117743648340306... Accuracy: 0.9388333559036255\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1310 - acc: 0.9413\n","Epoch: 165... Training loss: 0.13104825923591853... Accuracy: 0.9413333535194397\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1327 - acc: 0.9423\n","Epoch: 166... Training loss: 0.1326813780516386... Accuracy: 0.9423333406448364\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1325 - acc: 0.9427\n","Epoch: 167... Training loss: 0.13253687154253324... Accuracy: 0.9426666498184204\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1413 - acc: 0.9386\n","Epoch: 168... Training loss: 0.1413144311706225... Accuracy: 0.9385833144187927\n","12000/12000 [==============================] - 1s 64us/sample - loss: 0.1346 - acc: 0.9413\n","Epoch: 169... Training loss: 0.13455847839514415... Accuracy: 0.9413333535194397\n","12000/12000 [==============================] - 1s 58us/sample - loss: 0.1374 - acc: 0.9387\n","Epoch: 170... Training loss: 0.13742542942861716... Accuracy: 0.9386666417121887\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1336 - acc: 0.9419\n","Epoch: 171... Training loss: 0.13355852887531122... Accuracy: 0.9419166445732117\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1289 - acc: 0.9437\n","Epoch: 172... Training loss: 0.1289279399837057... Accuracy: 0.9436666369438171\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1281 - acc: 0.9455\n","Epoch: 173... Training loss: 0.12810093370825051... Accuracy: 0.9455000162124634\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1275 - acc: 0.9448\n","Epoch: 174... Training loss: 0.12754783941060305... Accuracy: 0.9448333382606506\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1338 - acc: 0.9420\n","Epoch: 175... Training loss: 0.13383604780832928... Accuracy: 0.9419999718666077\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1342 - acc: 0.9415\n","Epoch: 176... Training loss: 0.13416116986672083... Accuracy: 0.9415000081062317\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1296 - acc: 0.9431\n","Epoch: 177... Training loss: 0.1295827713708083... Accuracy: 0.9430833458900452\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1321 - acc: 0.9417\n","Epoch: 178... Training loss: 0.1321487803310156... Accuracy: 0.9417499899864197\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1417 - acc: 0.9386\n","Epoch: 179... Training loss: 0.1416518116692702... Accuracy: 0.9385833144187927\n","12000/12000 [==============================] - 1s 62us/sample - loss: 0.1294 - acc: 0.9445\n","Epoch: 180... Training loss: 0.12944627439479034... Accuracy: 0.9445000290870667\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1369 - acc: 0.9400\n","Epoch: 181... Training loss: 0.13690652158856392... Accuracy: 0.9399999976158142\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1316 - acc: 0.9413\n","Epoch: 182... Training loss: 0.13161233250796794... Accuracy: 0.9412500262260437\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1296 - acc: 0.9443\n","Epoch: 183... Training loss: 0.1295810402855277... Accuracy: 0.9443333148956299\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1271 - acc: 0.9450\n","Epoch: 184... Training loss: 0.12707393755018712... Accuracy: 0.9449999928474426\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1325 - acc: 0.9421\n","Epoch: 185... Training loss: 0.13248637600739796... Accuracy: 0.9420833587646484\n","12000/12000 [==============================] - 1s 61us/sample - loss: 0.1289 - acc: 0.9441\n","Epoch: 186... Training loss: 0.1289121015003572... Accuracy: 0.9440833330154419\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1274 - acc: 0.9449\n","Epoch: 187... Training loss: 0.127352733515203... Accuracy: 0.9449166655540466\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1292 - acc: 0.9423\n","Epoch: 188... Training loss: 0.12918452810744444... Accuracy: 0.9422500133514404\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1288 - acc: 0.9434\n","Epoch: 189... Training loss: 0.12875972137227654... Accuracy: 0.9434166550636292\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1292 - acc: 0.9426\n","Epoch: 190... Training loss: 0.12922163926064967... Accuracy: 0.9425833225250244\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1338 - acc: 0.9396\n","Epoch: 191... Training loss: 0.13378189558784168... Accuracy: 0.9395833611488342\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1379 - acc: 0.9392\n","Epoch: 192... Training loss: 0.13789292691648006... Accuracy: 0.9392499923706055\n","12000/12000 [==============================] - 1s 58us/sample - loss: 0.1349 - acc: 0.9405\n","Epoch: 193... Training loss: 0.1349233199506998... Accuracy: 0.940500020980835\n","12000/12000 [==============================] - 1s 62us/sample - loss: 0.1307 - acc: 0.9440\n","Epoch: 194... Training loss: 0.13069006924827895... Accuracy: 0.9440000057220459\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1335 - acc: 0.9420\n","Epoch: 195... Training loss: 0.13352760178099077... Accuracy: 0.9419999718666077\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1273 - acc: 0.9448\n","Epoch: 196... Training loss: 0.12732277704775333... Accuracy: 0.9447500109672546\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1269 - acc: 0.9453\n","Epoch: 197... Training loss: 0.12692847146466374... Accuracy: 0.9453333616256714\n","12000/12000 [==============================] - 1s 59us/sample - loss: 0.1285 - acc: 0.9444\n","Epoch: 198... Training loss: 0.12847809932629267... Accuracy: 0.9444166421890259\n","12000/12000 [==============================] - 1s 60us/sample - loss: 0.1236 - acc: 0.9466\n","Epoch: 199... Training loss: 0.1235516530436774... Accuracy: 0.9465833306312561\n","Confusion matrix:\n","[[1444   43    0]\n"," [  43 1013   52]\n"," [   0   31  374]]\n","Accuracy: \n","0.9436666666666667\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"ywAI6WfHujxe","colab_type":"text"},"source":["### MNIST Classification"]},{"cell_type":"code","metadata":{"id":"15_FXzMGVK1j","colab_type":"code","colab":{}},"source":["def tf_mnist_classification():\n","    # Load data from file\n","    # Make sure that fashion-mnist/*.gz is in data/\n","    train_X, train_Y, val_X, val_Y, test_X, test_Y = get_mnist_data(1)\n","    train_X, val_X, test_X = normalize(train_X, val_X, test_X)    \n","    \n","    num_class = (np.unique(train_Y)).shape[0]\n","\n","    # Pad 1 as the third feature of train_x and test_x\n","    train_X = add_one(train_X)\n","    val_X = add_one(val_X)\n","    test_X = add_one(test_X)\n","    \n","    train_Y = create_one_hot(train_Y, num_class)\n","    val_Y = create_one_hot(val_Y, num_class)\n","\n","    # Define hyper-parameters and train-related parameters\n","    hidden_layers = [128, 256, 100, 64]\n","    learning_rate = 0.001 # 0.01\n","    epochs = 50\n","    activation = tf.nn.relu\n","    optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n","\n","    #Initialize model\n","    dnn = DNNModel(hidden_layers=hidden_layers, num_classes=num_class,\n","                   activation=activation, epochs=epochs, optimizer=optimizer)\n","    #Train\n","    dnn.train(train_X, train_Y)\n","\n","    # TEST\n","    # Confusion matrix\n","    metrics = confusion_matrix(test_Y, dnn.predict(test_X))\n","    print('Confusion matrix:')\n","    print(metrics)\n","    print(\"Accuracy: \")\n","    print(metrics.trace()/test_Y.shape[0])"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vv9L55P8VK1l","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"9433f76c-6508-40e9-bf7a-5c391a52097d","executionInfo":{"status":"ok","timestamp":1569298521323,"user_tz":-420,"elapsed":1654959,"user":{"displayName":"Dang Nguyen Minh","photoUrl":"","userId":"06404449821748651554"}}},"source":["tf_mnist_classification()"],"execution_count":24,"outputs":[{"output_type":"stream","text":["Reading fashion MNIST data...\n","Done reading\n","50000/50000 [==============================] - 3s 69us/sample - loss: 0.4809 - acc: 0.8243\n","Epoch: 0... Training loss: 0.48090993117809294... Accuracy: 0.8242999911308289\n","50000/50000 [==============================] - 3s 68us/sample - loss: 0.3665 - acc: 0.8659\n","Epoch: 1... Training loss: 0.36647848952770234... Accuracy: 0.8659200072288513\n","50000/50000 [==============================] - 3s 66us/sample - loss: 0.3292 - acc: 0.8778\n","Epoch: 2... Training loss: 0.32923464332580565... Accuracy: 0.8778200149536133\n","50000/50000 [==============================] - 3s 68us/sample - loss: 0.3051 - acc: 0.8875\n","Epoch: 3... Training loss: 0.30507787571430206... Accuracy: 0.887499988079071\n","50000/50000 [==============================] - 3s 67us/sample - loss: 0.2860 - acc: 0.8932\n","Epoch: 4... Training loss: 0.28599215045690535... Accuracy: 0.8931800127029419\n","50000/50000 [==============================] - 3s 68us/sample - loss: 0.2694 - acc: 0.9008\n","Epoch: 5... Training loss: 0.26944874169826505... Accuracy: 0.9008399844169617\n","50000/50000 [==============================] - 3s 68us/sample - loss: 0.2553 - acc: 0.9035\n","Epoch: 6... Training loss: 0.25527309998512265... Accuracy: 0.9034799933433533\n","50000/50000 [==============================] - 3s 67us/sample - loss: 0.2410 - acc: 0.9086\n","Epoch: 7... Training loss: 0.24103177552819252... Accuracy: 0.9086199998855591\n","50000/50000 [==============================] - 3s 67us/sample - loss: 0.2340 - acc: 0.9124\n","Epoch: 8... Training loss: 0.2340430740237236... Accuracy: 0.9123799800872803\n","50000/50000 [==============================] - 3s 69us/sample - loss: 0.2199 - acc: 0.9173\n","Epoch: 9... Training loss: 0.21986373425006867... Accuracy: 0.9173200130462646\n","50000/50000 [==============================] - 3s 67us/sample - loss: 0.2122 - acc: 0.9196\n","Epoch: 10... Training loss: 0.21219406068444252... Accuracy: 0.9196199774742126\n","50000/50000 [==============================] - 3s 68us/sample - loss: 0.2030 - acc: 0.9234\n","Epoch: 11... Training loss: 0.20303436097860336... Accuracy: 0.9233599901199341\n","50000/50000 [==============================] - 3s 69us/sample - loss: 0.2004 - acc: 0.9249\n","Epoch: 12... Training loss: 0.20036532465100287... Accuracy: 0.9249399900436401\n","50000/50000 [==============================] - 3s 68us/sample - loss: 0.1905 - acc: 0.9269\n","Epoch: 13... Training loss: 0.19050685967564582... Accuracy: 0.9268800020217896\n","50000/50000 [==============================] - 3s 68us/sample - loss: 0.1833 - acc: 0.9306\n","Epoch: 14... Training loss: 0.18332774781227112... Accuracy: 0.930620014667511\n","50000/50000 [==============================] - 3s 67us/sample - loss: 0.1776 - acc: 0.9336\n","Epoch: 15... Training loss: 0.17762218132451177... Accuracy: 0.9335799813270569\n","50000/50000 [==============================] - 3s 67us/sample - loss: 0.1697 - acc: 0.9358\n","Epoch: 16... Training loss: 0.16967213406562806... Accuracy: 0.9358000159263611\n","50000/50000 [==============================] - 3s 67us/sample - loss: 0.1706 - acc: 0.9360\n","Epoch: 17... Training loss: 0.17058911803662777... Accuracy: 0.9359599947929382\n","50000/50000 [==============================] - 3s 66us/sample - loss: 0.1600 - acc: 0.9398\n","Epoch: 18... Training loss: 0.1600274617767334... Accuracy: 0.9397799968719482\n","50000/50000 [==============================] - 3s 67us/sample - loss: 0.1576 - acc: 0.9403\n","Epoch: 19... Training loss: 0.15757177155196667... Accuracy: 0.9403200149536133\n","50000/50000 [==============================] - 3s 67us/sample - loss: 0.1502 - acc: 0.9434\n","Epoch: 20... Training loss: 0.15015711108237506... Accuracy: 0.9434000253677368\n","50000/50000 [==============================] - 3s 68us/sample - loss: 0.1451 - acc: 0.9452\n","Epoch: 21... Training loss: 0.14512897430717944... Accuracy: 0.9451599717140198\n","50000/50000 [==============================] - 3s 67us/sample - loss: 0.1429 - acc: 0.9458\n","Epoch: 22... Training loss: 0.14285146750323474... Accuracy: 0.9457600116729736\n","50000/50000 [==============================] - 3s 68us/sample - loss: 0.1427 - acc: 0.9469\n","Epoch: 23... Training loss: 0.1426859048768878... Accuracy: 0.9468600153923035\n","50000/50000 [==============================] - 3s 67us/sample - loss: 0.1354 - acc: 0.9491\n","Epoch: 24... Training loss: 0.13544723276779055... Accuracy: 0.9490600228309631\n","50000/50000 [==============================] - 3s 67us/sample - loss: 0.1289 - acc: 0.9508\n","Epoch: 25... Training loss: 0.12893809046521784... Accuracy: 0.9508200287818909\n","50000/50000 [==============================] - 3s 67us/sample - loss: 0.1305 - acc: 0.9505\n","Epoch: 26... Training loss: 0.13054277896404268... Accuracy: 0.9504600167274475\n","50000/50000 [==============================] - 3s 67us/sample - loss: 0.1216 - acc: 0.9549\n","Epoch: 27... Training loss: 0.121569049481377... Accuracy: 0.9548599720001221\n","50000/50000 [==============================] - 3s 67us/sample - loss: 0.1250 - acc: 0.9537\n","Epoch: 28... Training loss: 0.1250285354180634... Accuracy: 0.9536799788475037\n","50000/50000 [==============================] - 3s 66us/sample - loss: 0.1120 - acc: 0.9587\n","Epoch: 29... Training loss: 0.11201497799322009... Accuracy: 0.9587000012397766\n","50000/50000 [==============================] - 3s 67us/sample - loss: 0.1137 - acc: 0.9567\n","Epoch: 30... Training loss: 0.11374291488967836... Accuracy: 0.9567400217056274\n","50000/50000 [==============================] - 3s 66us/sample - loss: 0.1096 - acc: 0.9591\n","Epoch: 31... Training loss: 0.10963668924905359... Accuracy: 0.9591000080108643\n","50000/50000 [==============================] - 3s 67us/sample - loss: 0.1109 - acc: 0.9587\n","Epoch: 32... Training loss: 0.11086456205055117... Accuracy: 0.9586799740791321\n","50000/50000 [==============================] - 3s 67us/sample - loss: 0.1089 - acc: 0.9582\n","Epoch: 33... Training loss: 0.10893761720720678... Accuracy: 0.9581800103187561\n","50000/50000 [==============================] - 3s 67us/sample - loss: 0.1046 - acc: 0.9624\n","Epoch: 34... Training loss: 0.10462225367825478... Accuracy: 0.9624000191688538\n","50000/50000 [==============================] - 3s 67us/sample - loss: 0.1051 - acc: 0.9608\n","Epoch: 35... Training loss: 0.10508461660966277... Accuracy: 0.9608399868011475\n","50000/50000 [==============================] - 3s 69us/sample - loss: 0.1008 - acc: 0.9625\n","Epoch: 36... Training loss: 0.10084517232872546... Accuracy: 0.9624800086021423\n","50000/50000 [==============================] - 3s 69us/sample - loss: 0.1010 - acc: 0.9623\n","Epoch: 37... Training loss: 0.10098779054157436... Accuracy: 0.9623000025749207\n","50000/50000 [==============================] - 3s 70us/sample - loss: 0.0935 - acc: 0.9661\n","Epoch: 38... Training loss: 0.09347683729026467... Accuracy: 0.9660999774932861\n","50000/50000 [==============================] - 3s 68us/sample - loss: 0.0928 - acc: 0.9648\n","Epoch: 39... Training loss: 0.09284772971903905... Accuracy: 0.9648000001907349\n","50000/50000 [==============================] - 3s 68us/sample - loss: 0.0944 - acc: 0.9642\n","Epoch: 40... Training loss: 0.09443797673134599... Accuracy: 0.9642400145530701\n","50000/50000 [==============================] - 3s 67us/sample - loss: 0.0916 - acc: 0.9661\n","Epoch: 41... Training loss: 0.0916396815944463... Accuracy: 0.9660599827766418\n","50000/50000 [==============================] - 3s 67us/sample - loss: 0.0926 - acc: 0.9661\n","Epoch: 42... Training loss: 0.09261438567064703... Accuracy: 0.9660999774932861\n","50000/50000 [==============================] - 3s 67us/sample - loss: 0.0854 - acc: 0.9686\n","Epoch: 43... Training loss: 0.08539980880772695... Accuracy: 0.9686400294303894\n","50000/50000 [==============================] - 3s 68us/sample - loss: 0.0878 - acc: 0.9682\n","Epoch: 44... Training loss: 0.08780702078885398... Accuracy: 0.9681800007820129\n","50000/50000 [==============================] - 3s 67us/sample - loss: 0.0882 - acc: 0.9685\n","Epoch: 45... Training loss: 0.0882224494529143... Accuracy: 0.968500018119812\n","50000/50000 [==============================] - 3s 67us/sample - loss: 0.0806 - acc: 0.9711\n","Epoch: 46... Training loss: 0.08062998842837289... Accuracy: 0.9711199998855591\n","50000/50000 [==============================] - 3s 68us/sample - loss: 0.0853 - acc: 0.9684\n","Epoch: 47... Training loss: 0.08529172163657844... Accuracy: 0.9683799743652344\n","50000/50000 [==============================] - 3s 68us/sample - loss: 0.0867 - acc: 0.9692\n","Epoch: 48... Training loss: 0.0867167387109442... Accuracy: 0.9692400097846985\n","50000/50000 [==============================] - 3s 68us/sample - loss: 0.0747 - acc: 0.9730\n","Epoch: 49... Training loss: 0.0746544784299098... Accuracy: 0.9729599952697754\n","Confusion matrix:\n","[[837   0  27  19   4   0 106   0   7   0]\n"," [  4 971   1  20   1   0   3   0   0   0]\n"," [ 25   1 859   8  36   0  69   0   2   0]\n"," [ 28   5  21 883  38   0  23   0   2   0]\n"," [  1   1 141  35 699   0 119   0   4   0]\n"," [  0   0   0   0   0 970   0  13   1  16]\n"," [123   0  88  27  34   0 722   0   6   0]\n"," [  0   0   0   0   0  19   0 957   0  24]\n"," [  4   0   5   5   6   2   8   4 966   0]\n"," [  1   0   0   0   0   8   0  36   0 955]]\n","Accuracy: \n","0.8819\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"id":"A796xi6AaXwy","colab_type":"text"},"source":["## III. Thang điểm\n","\n"]},{"cell_type":"markdown","metadata":{"id":"ojErsXU5xX6u","colab_type":"text"},"source":["| TODO  | Điểm  |   |   |   |\n","|---|---|---|---|---|\n","|\\[TODO 1\\] Cài đặt các hàm activation   |  1 |   |   |   |\n","|\\[TODO 2\\] Hàm `forward`/`HiddenLayer` | 1  |   |   |   |\n","|\\[TODO 3\\] Hàm `backward`/`HiddenLayer`  | 2  |   |   |   |\n","|\\[TODO 4\\] Hàm `forward`/`NeuralNetwork`  | 0.5  |   |   |   |\n","|\\[TODO 5\\] Hàm `compute_loss`/`NeuralNetwork`  | 1.5  |   |   |   |\n","|\\[TODO 6\\] Hàm `compute_delta_grad_last`/`NeuralNetwork`  | 1  |   |   |   |\n","|\\[TODO 7\\] Hàm `backward`/`NeuralNetwork`  | 1  |   |   |   |\n","|\\[TODO 8\\] Hàm `update_weight_momentum`/`NeuralNetwork`  | 1  |   |   |   |\n","|\\[TODO 9\\] Hàm `minibatch_train`  | 2  |   |   |   |\n","|\\[TODO 10\\] Cài đặt kiến trúc Neural network sử dụng tensorflow  | 1  |   |   |   |\n","|\\[TODO 11\\] Cài đặt hàm tính accuracy  | 1  |   |   |   |\n","|\\[TODO 12\\] Cài đặt hàm train trong class `DNNModel`  | 2  |   |   |   |\n","|**Tổng**| **15**  |   |   |   |"]},{"cell_type":"markdown","metadata":{"id":"UWgbEVveBxji","colab_type":"text"},"source":["## Author: Giang Tran, Hoa Nguyen"]},{"cell_type":"code","metadata":{"id":"AnHV18q7eVgA","colab_type":"code","colab":{}},"source":[""],"execution_count":0,"outputs":[]}]}